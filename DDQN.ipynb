{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kopok/RL-Experiments/blob/master/DDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJfghbV85Rbl",
        "colab_type": "code",
        "outputId": "997e5a49-02e2-428e-cb64-58ceda81b2c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import itertools\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import tflearn\n",
        "import tensorflow as tf\n",
        "from skimage.transform import rescale, resize, downscale_local_mean\n",
        "from skimage.color import rgb2gray\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "from importlib import reload  # Not needed in Python 2\n",
        "import logging\n",
        "reload(logging)\n",
        "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA6Kud9lDE99",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ENV_NAME = \"Pong-v0\"\n",
        "\n",
        "LEARNING_RATE = 0.00025\n",
        "\n",
        "MEMORY_SIZE = 15000\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "EXPLORATION_MAX = 1.0\n",
        "EXPLORATION_MIN = 0.01\n",
        "EXPLORATION_DECAY = 0.995\n",
        "IMG_WIDTH = 84\n",
        "IMG_HEIGHT = 84\n",
        "\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Control parameters\n",
        "MAX_EPISODE_LENGTH = 18000       # Equivalent of 5 minutes of gameplay at 60 frames per second\n",
        "EVAL_FREQUENCY = 200000          # Number of frames the agent sees between evaluations\n",
        "EVAL_STEPS = 10000               # Number of frames for one evaluation\n",
        "NETW_UPDATE_FREQ = 10000         # Number of chosen actions between updating the target network. \n",
        "                                 # According to Mnih et al. 2015 this is measured in the number of \n",
        "                                 # parameter updates (every four actions), however, in the \n",
        "                                 # DeepMind code, it is clearly measured in the number\n",
        "                                 # of actions the agent choses\n",
        "DISCOUNT_FACTOR = 0.99           # gamma in the Bellman equation\n",
        "REPLAY_MEMORY_START_SIZE = MEMORY_SIZE # Number of completely random actions, \n",
        "                                 # before the agent starts learning\n",
        "MAX_FRAMES = 30000000            # Total number of frames the agent sees \n",
        "NO_OP_STEPS = 10                 # Number of 'NOOP' or 'FIRE' actions at the beginning of an \n",
        "                                 # evaluation episode\n",
        "UPDATE_FREQ = 4                  # Every four actions a gradient descend step is performed\n",
        "HIDDEN = 1024                    # Number of filters in the final convolutional layer. The output \n",
        "                                 # has the shape (1,1,1024) which is split into two streams. Both \n",
        "                                 # the advantage stream and value stream have the shape \n",
        "                                 # (1,1,512). This is slightly different from the original \n",
        "                                 # implementation but tests I did with the environment Pong \n",
        "                                 # have shown that this way the score increases more quickly\n",
        "                                 # Hessel et al. 2017 used 0.0000625\n",
        "\n",
        "RUNID = 'run_1'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl1hw2RV2xbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(state):\n",
        "    # Turn to grey\n",
        "    state = rgb2gray(state)\n",
        "    \n",
        "    # Crop 26px from top & 16px from bottom\n",
        "    state = state[26:-16]\n",
        "    \n",
        "    # Downscale 168, 160 to 84,84\n",
        "    state = resize(state, [IMG_HEIGHT, IMG_WIDTH], anti_aliasing=False)\n",
        "    tf.image.resize_images(state, [IMG_HEIGHT, IMG_WIDTH], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    #plt.imshow(state, cmap=\"gray\")\n",
        "    #plt.show()\n",
        "    \n",
        "    \n",
        "    # Reshape\n",
        "    state = np.array(state).reshape([-1, IMG_HEIGHT, IMG_WIDTH, 1])\n",
        "    return state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9h9YIrCL2aIT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQN(object):\n",
        "    \"\"\"Implements a Deep Q Network\"\"\"\n",
        "    \n",
        "    # pylint: disable=too-many-instance-attributes\n",
        "    \n",
        "    def __init__(self, n_actions, hidden=1024, learning_rate=0.00001, \n",
        "                 frame_height=84, frame_width=84, agent_history_length=4, MEMORY_SIZE = 8000):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            n_actions: Integer, number of possible actions\n",
        "            hidden: Integer, Number of filters in the final convolutional layer. \n",
        "                    This is different from the DeepMind implementation\n",
        "            learning_rate: Float, Learning rate for the Adam optimizer\n",
        "            frame_height: Integer, Height of a frame of an Atari game\n",
        "            frame_width: Integer, Width of a frame of an Atari game\n",
        "            agent_history_length: Integer, Number of frames stacked together to create a state\n",
        "        \"\"\"     \n",
        "        \n",
        "        self.n_actions = n_actions\n",
        "        self.hidden = hidden\n",
        "        self.learning_rate = learning_rate\n",
        "        self.frame_height = frame_height\n",
        "        self.frame_width = frame_width\n",
        "        self.agent_history_length = agent_history_length\n",
        "        self.exploration_rate = EXPLORATION_MAX\n",
        "\n",
        "        \n",
        "        self.input = tf.placeholder(shape=[None, self.frame_height, \n",
        "                                           self.frame_width, self.agent_history_length], \n",
        "                                    dtype=tf.float32)\n",
        "        # Normalizing the input\n",
        "        self.inputscaled = self.input/255\n",
        "        \n",
        "        # Convolutional layers\n",
        "        self.conv1 = tf.layers.conv2d(\n",
        "            inputs=self.inputscaled, filters=32, kernel_size=[8, 8], strides=4,\n",
        "            kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
        "            padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv1')\n",
        "        self.conv2 = tf.layers.conv2d(\n",
        "            inputs=self.conv1, filters=64, kernel_size=[4, 4], strides=2, \n",
        "            kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
        "            padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv2')\n",
        "        self.conv3 = tf.layers.conv2d(\n",
        "            inputs=self.conv2, filters=64, kernel_size=[3, 3], strides=1, \n",
        "            kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
        "            padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv3')\n",
        "        self.conv4 = tf.layers.conv2d(\n",
        "            inputs=self.conv3, filters=hidden, kernel_size=[7, 7], strides=1, \n",
        "            kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
        "            padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv4')\n",
        "        \n",
        "        # Splitting into value and advantage stream\n",
        "        self.valuestream, self.advantagestream = tf.split(self.conv4, 2, 3)\n",
        "        self.valuestream = tf.layers.flatten(self.valuestream)\n",
        "        self.advantagestream = tf.layers.flatten(self.advantagestream)\n",
        "        self.advantage = tf.layers.dense(\n",
        "            inputs=self.advantagestream, units=self.n_actions,\n",
        "            kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"advantage\")\n",
        "        self.value = tf.layers.dense(\n",
        "            inputs=self.valuestream, units=1, \n",
        "            kernel_initializer=tf.variance_scaling_initializer(scale=2), name='value')\n",
        "        \n",
        "        # Combining value and advantage into Q-values\n",
        "        self.q_values = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage, axis=1, keepdims=True))\n",
        "        self.best_action = tf.argmax(self.q_values, 1)\n",
        "        \n",
        "        # targetQ according to Bellman equation: \n",
        "        # Q = r + gamma*max Q', calculated in the function learn()\n",
        "        self.target_q = tf.placeholder(shape=[None], dtype=tf.float32)\n",
        "        # Action that was performed\n",
        "        self.action = tf.placeholder(shape=[None], dtype=tf.int32)\n",
        "        # Q value of the action that was performed\n",
        "        self.Q = tf.reduce_sum(tf.multiply(self.q_values, tf.one_hot(self.action, self.n_actions, dtype=tf.float32)), axis=1)\n",
        "        \n",
        "        # Parameter updates\n",
        "        self.loss = tf.reduce_mean(tf.losses.huber_loss(labels=self.target_q, predictions=self.Q))\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "        self.update = self.optimizer.minimize(self.loss)\n",
        "        \n",
        "    def act(self,session, state):\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            return random.randrange(self.n_actions)\n",
        "\n",
        "        return session.run(self.best_action, feed_dict={self.input:[state]})[0]\n",
        "\n",
        "    def update_explore_rate(self):\n",
        "        self.exploration_rate *= EXPLORATION_DECAY\n",
        "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
        "        \n",
        "class Memory(object):\n",
        "  \n",
        "  \n",
        "      def __init__(self,init_state, MEMORY_SIZE = 8000, agent_history_length=4):\n",
        "      \n",
        "        self.agent_history_length = agent_history_length\n",
        "        self.frames  = deque(maxlen=MEMORY_SIZE)\n",
        "        self.actions = deque(maxlen=MEMORY_SIZE-agent_history_length)\n",
        "        self.rewards  = deque(maxlen=MEMORY_SIZE-agent_history_length)\n",
        "        self.terminals = deque(maxlen=MEMORY_SIZE-agent_history_length)\n",
        "        self.reset_points = []\n",
        "        \n",
        "        self.setup(init_state, False)\n",
        "          \n",
        "      \n",
        "      def setup(self, state, balance = True):\n",
        "          if state.shape != (84, 84):\n",
        "              print(\"Shape was: \"+str(state.shape) + \" Expected: [ 84, 84 ]\")\n",
        "              raise IndexError\n",
        "          \n",
        "          \n",
        "          \n",
        "          if balance:\n",
        "              for i in range(self.agent_history_length):\n",
        "                self.remember(state,None,None,None)\n",
        "                \n",
        "          else:\n",
        "            self.frames.extend([state, state, state, state])\n",
        "            \n",
        "          self.reset_points.append(len(self.actions))\n",
        "                    \n",
        "      def remember(self, new_frame, action, reward, terminal):         \n",
        "          \n",
        "          # We are about to start dropping items, we need to update idx positions\n",
        "          if len(self.frames) == self.frames.maxlen:\n",
        "              i = 0\n",
        "              dropped = 0\n",
        "              while i < len(self.reset_points):\n",
        "            \n",
        "                  self.reset_points[i] -= 1\n",
        "              \n",
        "                  if self.reset_points[i] < 0:\n",
        "                      dropped = 1\n",
        "                      self.reset_points.pop(0)\n",
        "                  else:\n",
        "                    i+=1\n",
        "\n",
        "                             \n",
        "          if new_frame.shape != (84,84):\n",
        "            print(\"Shape was: \" + str(new_frame.shape) + \" Expected: [ 84, 84 ]\")\n",
        "            raise IndexError\n",
        "            \n",
        "          self.frames.append(new_frame)\n",
        "          self.actions.append(action)\n",
        "          self.rewards.append(reward)\n",
        "          self.terminals.append(terminal)\n",
        "\n",
        "          \n",
        "      def check_idx(self, idx):\n",
        "          if len(self.reset_points) > 0:\n",
        "              i = 0           \n",
        "              reset_point = self.reset_points[i]\n",
        "                             \n",
        "              while idx > reset_point and i+1 < len(self.reset_points):\n",
        "                  i+=1\n",
        "                  reset_point = self.reset_points[i]\n",
        "            \n",
        "              if idx < reset_point and idx >= reset_point - self.agent_history_length:\n",
        "                  return False\n",
        "          return True\n",
        "                             \n",
        "      def get_memory(self, idx):\n",
        "          if idx >= len(self.frames) or not self.check_idx(idx):\n",
        "            raise IndexError\n",
        "\n",
        "          s = []\n",
        "          a = self.actions[idx]\n",
        "          r = self.rewards[idx]\n",
        "          s_ = []\n",
        "          t = self.terminals[idx]\n",
        "          \n",
        "          for i in range(idx,idx+self.agent_history_length):\n",
        "            s.append(self.frames[i])\n",
        "            s_.append(self.frames[i+1])\n",
        "          \n",
        "          \n",
        "          \n",
        "          s1 = np.stack([s[0],s[1],s[2],s[3]], -1) \n",
        "          s2 = np.stack([s_[0],s_[1],s_[2],s_[3]], -1 )\n",
        "          \n",
        "          \n",
        "          return s1,a,r,s2,t\n",
        "      \n",
        "\n",
        "      def get_batch(self, size=1):\n",
        "          s,a,r,s_,t = [],[],[],[],[]\n",
        "\n",
        "          for i in range(size):\n",
        "          \n",
        "            idx = random.randrange(len(self.actions))\n",
        "            while not self.check_idx(idx):\n",
        "              idx = random.randrange(len(self.actions))\n",
        "            \n",
        "            sI,aI,rI,s_I,tI = self.get_memory(idx)\n",
        "            \n",
        "            errors = \"\"\n",
        "            if aI == None: errors+= \"Action \"\n",
        "            if rI == None: errors+= \"reward \"\n",
        "            if tI == None: errors+= \"terminal \"\n",
        "            \n",
        "            if len(errors) > 0:\n",
        "              print(errors)\n",
        "              print(idx,self.check_idx(idx))\n",
        "              print(self.reset_points)\n",
        "              print(list(itertools.islice(self.terminals, idx-5, idx+5)))\n",
        "              print(list(itertools.islice(self.actions, idx-5, idx+5)))\n",
        "              print(list(itertools.islice(self.rewards, idx-5, idx+5)))\n",
        "              \n",
        "              raise TypeError\n",
        "\n",
        "\n",
        "            s.append(sI)\n",
        "            a.append(aI)\n",
        "            r.append(rI)\n",
        "            s_.append(s_I)\n",
        "            t.append(tI)\n",
        "          \n",
        "          return np.array(s), np.array(a), np.array(r), np.array(s_), np.array(t) \n",
        "          \n",
        "      def get_curr_state(self):\n",
        "        \n",
        "          state = []\n",
        "        \n",
        "          l = len(self.frames)\n",
        "          for i in range(l-1,l-self.agent_history_length-1,-1):\n",
        "              item = self.frames[i]\n",
        "              state.append(item)\n",
        "          \n",
        "          return state\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm-vmBkUF9fo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learn(session, memory, main_dqn, target_dqn, batch_size, gamma):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        session: A tensorflow sesson object\n",
        "        replay_memory: A Memory object\n",
        "        main_dqn: A DQN object\n",
        "        target_dqn: A DQN object\n",
        "        batch_size: Integer, Batch size\n",
        "        gamma: Float, discount factor for the Bellman equation\n",
        "    Returns:\n",
        "        loss: The loss of the minibatch, for tensorboard\n",
        "    Draws a minibatch from the replay memory, calculates the \n",
        "    target Q-value that the prediction Q-value is regressed to. \n",
        "    Then a parameter update is performed on the main DQN.\n",
        "    \"\"\"\n",
        "    # Draw a minibatch from the replay memory\n",
        "    states, actions, rewards, new_states, terminal_flags = memory.get_batch(batch_size)    \n",
        "\n",
        "    # The main network estimates which action is best (in the next state s', new_states is passed!) \n",
        "    # for every transition in the minibatch\n",
        "    arg_q_max = session.run(main_dqn.best_action, feed_dict={main_dqn.input:new_states})\n",
        "\n",
        "    # The target network estimates the Q-values (in the next state s', new_states is passed!) \n",
        "    # for every transition in the minibatch\n",
        "    q_vals = session.run(target_dqn.q_values, feed_dict={target_dqn.input:new_states})\n",
        "    double_q = q_vals[range(batch_size), arg_q_max]\n",
        "    \n",
        "    # Bellman equation. Multiplication with (1-terminal_flags) makes sure that \n",
        "    # if the game is over, targetQ=rewards\n",
        "    \n",
        "    \n",
        "    inverted_flags = 1-terminal_flags\n",
        "    discounted_Q = gamma*double_q\n",
        "    discounted_finshed_Q = discounted_Q * inverted_flags\n",
        "    \n",
        "    \n",
        "    target_q = rewards + discounted_finshed_Q\n",
        "    \n",
        "    # Gradient descend step to update the parameters of the main network\n",
        "    loss, _ = session.run([main_dqn.loss, main_dqn.update], \n",
        "                          feed_dict={main_dqn.input:states, \n",
        "                                     main_dqn.target_q:target_q, \n",
        "                                     main_dqn.action:actions})\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7BTez8_GC83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TargetNetworkUpdater(object):\n",
        "    \"\"\"Copies the parameters of the main DQN to the target DQN\"\"\"\n",
        "    def __init__(self, main_dqn_vars, target_dqn_vars):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            main_dqn_vars: A list of tensorflow variables belonging to the main DQN network\n",
        "            target_dqn_vars: A list of tensorflow variables belonging to the target DQN network\n",
        "        \"\"\"\n",
        "        self.main_dqn_vars = main_dqn_vars\n",
        "        self.target_dqn_vars = target_dqn_vars\n",
        "\n",
        "    def _update_target_vars(self):\n",
        "        update_ops = []\n",
        "        for i, var in enumerate(self.main_dqn_vars):\n",
        "            copy_op = self.target_dqn_vars[i].assign(var.value())\n",
        "            update_ops.append(copy_op)\n",
        "        return update_ops\n",
        "            \n",
        "    def __call__(self, sess):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sess: A Tensorflow session object\n",
        "        Assigns the values of the parameters of the main network to the \n",
        "        parameters of the target network\n",
        "        \"\"\"\n",
        "        update_ops = self._update_target_vars()\n",
        "        for copy_op in update_ops:\n",
        "            sess.run(copy_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7_5BSkPGSqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def clip_reward(reward):\n",
        "    if reward > 0:\n",
        "        return 1\n",
        "    elif reward == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return -1\n",
        "      \n",
        "def preprocess(state):\n",
        "    # Turn to grey\n",
        "    state = rgb2gray(state)\n",
        "    \n",
        "    # Crop 26px from top & 16px from bottom\n",
        "    state = state[26:-16]\n",
        "    \n",
        "    # Downscale 168, 160 \n",
        "    state = resize(state, [IMG_HEIGHT, IMG_WIDTH], anti_aliasing=False)\n",
        "\n",
        "    #plt.imshow(state, cmap=\"gray\")\n",
        "    #plt.show()\n",
        "    \n",
        "    \n",
        "    # Reshape\n",
        "   # state = np.array(state).reshape([-1, IMG_HEIGHT, IMG_WIDTH])\n",
        "    return state\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "def train():\n",
        "  \n",
        "    env = gym.make(ENV_NAME)\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    # main DQN and target DQN networks:\n",
        "    with tf.variable_scope('mainDQN'):\n",
        "        MAIN_DQN = DQN(env.action_space.n, HIDDEN, LEARNING_RATE)   # (★★)\n",
        "    with tf.variable_scope('targetDQN'):\n",
        "        TARGET_DQN = DQN(env.action_space.n, HIDDEN)               # (★★)\n",
        "    \n",
        "    #env = env.unwrapped\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "    saver = tf.train.Saver()    \n",
        "\n",
        "    MAIN_DQN_VARS = tf.trainable_variables(scope='mainDQN')\n",
        "    TARGET_DQN_VARS = tf.trainable_variables(scope='targetDQN')\n",
        "  \n",
        "    update_networks = TargetNetworkUpdater(MAIN_DQN_VARS, TARGET_DQN_VARS)\n",
        "    memory = Memory(preprocess(env.reset()), MEMORY_SIZE)   \n",
        "\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(init)\n",
        "        run = 0\n",
        "        frame_number = 0\n",
        "  #      saver.restore(sess, \"/content/gdrive/My Drive/Models/DDQN/checkpoint\"+str(run)+\"-\"+str(frame_number))\n",
        "\n",
        "        rewards = []\n",
        "        loss_list = []\n",
        "        for i in range(run):\n",
        "          MAIN_DQN.update_explore_rate()\n",
        "          TARGET_DQN.update_explore_rate()\n",
        "        \n",
        "        while run < 3000:\n",
        "            run+=1\n",
        "            ########################\n",
        "            ####### Training #######\n",
        "            ########################\n",
        "            \n",
        "\n",
        "            \n",
        "            frame = preprocess(env.reset())\n",
        "            \n",
        "            if run > 1 :\n",
        "                memory.setup(frame)\n",
        "            \n",
        "            episode_reward_sum = 0\n",
        "            \n",
        "            \n",
        "            for _ in range(MAX_EPISODE_LENGTH):\n",
        "              \n",
        "                frame_number += 1\n",
        "              \n",
        "                state = np.stack(memory.get_curr_state(), -1)\n",
        "\n",
        "                action = MAIN_DQN.act(sess, state)   \n",
        "\n",
        "                unprocessed_new_frame, reward, terminal, _ = env.step(action)  \n",
        "                \n",
        "\n",
        "                # Clip the reward\n",
        "                clipped_reward = clip_reward(reward)\n",
        "\n",
        "                \n",
        "                memory.remember(preprocess(unprocessed_new_frame), action, reward, terminal)\n",
        "\n",
        "                episode_reward_sum += reward\n",
        "                   \n",
        "\n",
        "                if frame_number % UPDATE_FREQ == 0 and frame_number > REPLAY_MEMORY_START_SIZE:\n",
        "                    loss = learn(sess, memory, MAIN_DQN, TARGET_DQN,\n",
        "                                 BATCH_SIZE, gamma = DISCOUNT_FACTOR) # (8★)\n",
        "                    loss_list.append(loss)\n",
        "                    \n",
        "                if frame_number % NETW_UPDATE_FREQ == 0 and frame_number > REPLAY_MEMORY_START_SIZE:\n",
        "                    \n",
        "                    update_networks(sess) # (9★)\n",
        "\n",
        "                if terminal:\n",
        "                    output = (\"Run: \" + str(run) + \"  Reward: \" + str(episode_reward_sum) + \"  Explore Rate: \" + str(MAIN_DQN.exploration_rate) + \"  Frame Count: \"+ str(frame_number))\n",
        "\n",
        "                    logging.info(output)\n",
        "                    MAIN_DQN.update_explore_rate()\n",
        "                    TARGET_DQN.update_explore_rate()\n",
        "                    terminal = False\n",
        "                    break\n",
        "\n",
        "            rewards.append(episode_reward_sum)\n",
        "\n",
        "       \n",
        "            #Save the network parameters\n",
        "            if run%25 == 0:\n",
        "                saver.save(sess, \"/content/gdrive/My Drive/Models/DDQN/checkpoint\"+str(run), global_step=frame_number)\n",
        "            frames_for_gif = []\n",
        "            \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KER_AB5sPKq8",
        "colab_type": "code",
        "outputId": "caa885ef-7af1-4fba-ff5c-6400c59a1059",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train()"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:Run: 1  Reward: -16.0  Explore Rate: 1.0  Frame Count: 1803\n",
            "INFO:Run: 2  Reward: -20.0  Explore Rate: 0.995  Frame Count: 2937\n",
            "INFO:Run: 3  Reward: -21.0  Explore Rate: 0.990025  Frame Count: 4056\n",
            "INFO:Run: 4  Reward: -21.0  Explore Rate: 0.985074875  Frame Count: 5186\n",
            "INFO:Run: 5  Reward: -21.0  Explore Rate: 0.9801495006250001  Frame Count: 6323\n",
            "INFO:Run: 6  Reward: -21.0  Explore Rate: 0.9752487531218751  Frame Count: 7656\n",
            "INFO:Run: 7  Reward: -20.0  Explore Rate: 0.9703725093562657  Frame Count: 8892\n",
            "INFO:Run: 8  Reward: -21.0  Explore Rate: 0.9655206468094844  Frame Count: 10180\n",
            "INFO:Run: 9  Reward: -20.0  Explore Rate: 0.960693043575437  Frame Count: 11317\n",
            "INFO:Run: 10  Reward: -20.0  Explore Rate: 0.9558895783575597  Frame Count: 12449\n",
            "INFO:Run: 11  Reward: -20.0  Explore Rate: 0.9511101304657719  Frame Count: 13552\n",
            "INFO:Run: 12  Reward: -20.0  Explore Rate: 0.946354579813443  Frame Count: 14909\n",
            "INFO:Run: 13  Reward: -21.0  Explore Rate: 0.9416228069143757  Frame Count: 16215\n",
            "INFO:Run: 14  Reward: -21.0  Explore Rate: 0.9369146928798039  Frame Count: 17314\n",
            "INFO:Run: 15  Reward: -21.0  Explore Rate: 0.9322301194154049  Frame Count: 18408\n",
            "INFO:Run: 16  Reward: -19.0  Explore Rate: 0.9275689688183278  Frame Count: 19708\n",
            "INFO:Run: 17  Reward: -21.0  Explore Rate: 0.9229311239742362  Frame Count: 20891\n",
            "INFO:Run: 18  Reward: -21.0  Explore Rate: 0.918316468354365  Frame Count: 21939\n",
            "INFO:Run: 19  Reward: -20.0  Explore Rate: 0.9137248860125932  Frame Count: 23215\n",
            "INFO:Run: 20  Reward: -19.0  Explore Rate: 0.9091562615825302  Frame Count: 24615\n",
            "INFO:Run: 21  Reward: -21.0  Explore Rate: 0.9046104802746175  Frame Count: 25879\n",
            "INFO:Run: 22  Reward: -21.0  Explore Rate: 0.9000874278732445  Frame Count: 27191\n",
            "INFO:Run: 23  Reward: -18.0  Explore Rate: 0.8955869907338783  Frame Count: 28666\n",
            "INFO:Run: 24  Reward: -20.0  Explore Rate: 0.8911090557802088  Frame Count: 30183\n",
            "INFO:Run: 25  Reward: -20.0  Explore Rate: 0.8866535105013078  Frame Count: 31598\n",
            "INFO:Run: 26  Reward: -20.0  Explore Rate: 0.8822202429488013  Frame Count: 32991\n",
            "INFO:Run: 27  Reward: -21.0  Explore Rate: 0.8778091417340573  Frame Count: 34206\n",
            "INFO:Run: 28  Reward: -21.0  Explore Rate: 0.8734200960253871  Frame Count: 35501\n",
            "INFO:Run: 29  Reward: -19.0  Explore Rate: 0.8690529955452602  Frame Count: 36809\n",
            "INFO:Run: 30  Reward: -21.0  Explore Rate: 0.8647077305675338  Frame Count: 37816\n",
            "INFO:Run: 31  Reward: -21.0  Explore Rate: 0.8603841919146962  Frame Count: 38991\n",
            "INFO:Run: 32  Reward: -21.0  Explore Rate: 0.8560822709551227  Frame Count: 40124\n",
            "INFO:Run: 33  Reward: -21.0  Explore Rate: 0.851801859600347  Frame Count: 41187\n",
            "INFO:Run: 34  Reward: -20.0  Explore Rate: 0.8475428503023453  Frame Count: 42556\n",
            "INFO:Run: 35  Reward: -19.0  Explore Rate: 0.8433051360508336  Frame Count: 43962\n",
            "INFO:Run: 36  Reward: -19.0  Explore Rate: 0.8390886103705794  Frame Count: 45287\n",
            "INFO:Run: 37  Reward: -21.0  Explore Rate: 0.8348931673187264  Frame Count: 46343\n",
            "INFO:Run: 38  Reward: -21.0  Explore Rate: 0.8307187014821328  Frame Count: 47429\n",
            "INFO:Run: 39  Reward: -21.0  Explore Rate: 0.8265651079747222  Frame Count: 48451\n",
            "INFO:Run: 40  Reward: -20.0  Explore Rate: 0.8224322824348486  Frame Count: 49585\n",
            "INFO:Run: 41  Reward: -21.0  Explore Rate: 0.8183201210226743  Frame Count: 50730\n",
            "INFO:Run: 42  Reward: -21.0  Explore Rate: 0.8142285204175609  Frame Count: 51754\n",
            "INFO:Run: 43  Reward: -19.0  Explore Rate: 0.810157377815473  Frame Count: 53052\n",
            "INFO:Run: 44  Reward: -21.0  Explore Rate: 0.8061065909263957  Frame Count: 54067\n",
            "INFO:Run: 45  Reward: -21.0  Explore Rate: 0.8020760579717637  Frame Count: 55093\n",
            "INFO:Run: 46  Reward: -21.0  Explore Rate: 0.798065677681905  Frame Count: 56129\n",
            "INFO:Run: 47  Reward: -20.0  Explore Rate: 0.7940753492934954  Frame Count: 57308\n",
            "INFO:Run: 48  Reward: -21.0  Explore Rate: 0.7901049725470279  Frame Count: 58393\n",
            "INFO:Run: 49  Reward: -21.0  Explore Rate: 0.7861544476842928  Frame Count: 59493\n",
            "INFO:Run: 50  Reward: -21.0  Explore Rate: 0.7822236754458713  Frame Count: 60791\n",
            "INFO:Run: 51  Reward: -21.0  Explore Rate: 0.778312557068642  Frame Count: 61991\n",
            "INFO:Run: 52  Reward: -21.0  Explore Rate: 0.7744209942832988  Frame Count: 63356\n",
            "INFO:Run: 53  Reward: -19.0  Explore Rate: 0.7705488893118823  Frame Count: 64643\n",
            "INFO:Run: 54  Reward: -21.0  Explore Rate: 0.7666961448653229  Frame Count: 65829\n",
            "INFO:Run: 55  Reward: -21.0  Explore Rate: 0.7628626641409962  Frame Count: 67078\n",
            "INFO:Run: 56  Reward: -20.0  Explore Rate: 0.7590483508202912  Frame Count: 68346\n",
            "INFO:Run: 57  Reward: -21.0  Explore Rate: 0.7552531090661897  Frame Count: 69494\n",
            "INFO:Run: 58  Reward: -19.0  Explore Rate: 0.7514768435208588  Frame Count: 70768\n",
            "INFO:Run: 59  Reward: -19.0  Explore Rate: 0.7477194593032545  Frame Count: 72039\n",
            "INFO:Run: 60  Reward: -20.0  Explore Rate: 0.7439808620067382  Frame Count: 73193\n",
            "INFO:Run: 61  Reward: -20.0  Explore Rate: 0.7402609576967045  Frame Count: 74374\n",
            "INFO:Run: 62  Reward: -21.0  Explore Rate: 0.736559652908221  Frame Count: 75457\n",
            "INFO:Run: 63  Reward: -21.0  Explore Rate: 0.7328768546436799  Frame Count: 76579\n",
            "INFO:Run: 64  Reward: -21.0  Explore Rate: 0.7292124703704616  Frame Count: 77660\n",
            "INFO:Run: 65  Reward: -20.0  Explore Rate: 0.7255664080186093  Frame Count: 78868\n",
            "INFO:Run: 66  Reward: -21.0  Explore Rate: 0.7219385759785162  Frame Count: 79875\n",
            "INFO:Run: 67  Reward: -21.0  Explore Rate: 0.7183288830986236  Frame Count: 81136\n",
            "INFO:Run: 68  Reward: -21.0  Explore Rate: 0.7147372386831305  Frame Count: 82227\n",
            "INFO:Run: 69  Reward: -20.0  Explore Rate: 0.7111635524897149  Frame Count: 83561\n",
            "INFO:Run: 70  Reward: -21.0  Explore Rate: 0.7076077347272662  Frame Count: 84651\n",
            "INFO:Run: 71  Reward: -20.0  Explore Rate: 0.7040696960536299  Frame Count: 86043\n",
            "INFO:Run: 72  Reward: -18.0  Explore Rate: 0.7005493475733617  Frame Count: 87428\n",
            "INFO:Run: 73  Reward: -20.0  Explore Rate: 0.697046600835495  Frame Count: 88804\n",
            "INFO:Run: 74  Reward: -19.0  Explore Rate: 0.6935613678313175  Frame Count: 90168\n",
            "INFO:Run: 75  Reward: -21.0  Explore Rate: 0.6900935609921609  Frame Count: 91410\n",
            "INFO:Run: 76  Reward: -20.0  Explore Rate: 0.6866430931872001  Frame Count: 92651\n",
            "INFO:Run: 77  Reward: -20.0  Explore Rate: 0.6832098777212641  Frame Count: 93760\n",
            "INFO:Run: 78  Reward: -18.0  Explore Rate: 0.6797938283326578  Frame Count: 95511\n",
            "INFO:Run: 79  Reward: -21.0  Explore Rate: 0.6763948591909945  Frame Count: 96619\n",
            "INFO:Run: 80  Reward: -19.0  Explore Rate: 0.6730128848950395  Frame Count: 98165\n",
            "INFO:Run: 81  Reward: -19.0  Explore Rate: 0.6696478204705644  Frame Count: 99487\n",
            "INFO:Run: 82  Reward: -21.0  Explore Rate: 0.6662995813682115  Frame Count: 100580\n",
            "INFO:Run: 83  Reward: -21.0  Explore Rate: 0.6629680834613705  Frame Count: 101753\n",
            "INFO:Run: 84  Reward: -21.0  Explore Rate: 0.6596532430440636  Frame Count: 102786\n",
            "INFO:Run: 85  Reward: -21.0  Explore Rate: 0.6563549768288433  Frame Count: 103994\n",
            "INFO:Run: 86  Reward: -21.0  Explore Rate: 0.653073201944699  Frame Count: 105129\n",
            "INFO:Run: 87  Reward: -21.0  Explore Rate: 0.6498078359349755  Frame Count: 106237\n",
            "INFO:Run: 88  Reward: -20.0  Explore Rate: 0.6465587967553006  Frame Count: 107752\n",
            "INFO:Run: 89  Reward: -21.0  Explore Rate: 0.6433260027715241  Frame Count: 108787\n",
            "INFO:Run: 90  Reward: -20.0  Explore Rate: 0.6401093727576664  Frame Count: 110201\n",
            "INFO:Run: 91  Reward: -20.0  Explore Rate: 0.6369088258938781  Frame Count: 111378\n",
            "INFO:Run: 92  Reward: -21.0  Explore Rate: 0.6337242817644086  Frame Count: 112490\n",
            "INFO:Run: 93  Reward: -20.0  Explore Rate: 0.6305556603555866  Frame Count: 113690\n",
            "INFO:Run: 94  Reward: -21.0  Explore Rate: 0.6274028820538087  Frame Count: 114786\n",
            "INFO:Run: 95  Reward: -18.0  Explore Rate: 0.6242658676435396  Frame Count: 116220\n",
            "INFO:Run: 96  Reward: -20.0  Explore Rate: 0.6211445383053219  Frame Count: 117399\n",
            "INFO:Run: 97  Reward: -20.0  Explore Rate: 0.6180388156137953  Frame Count: 118608\n",
            "INFO:Run: 98  Reward: -21.0  Explore Rate: 0.6149486215357263  Frame Count: 119703\n",
            "INFO:Run: 99  Reward: -20.0  Explore Rate: 0.6118738784280476  Frame Count: 120856\n",
            "INFO:Run: 100  Reward: -21.0  Explore Rate: 0.6088145090359074  Frame Count: 121993\n",
            "INFO:Run: 101  Reward: -20.0  Explore Rate: 0.6057704364907278  Frame Count: 123116\n",
            "INFO:Run: 102  Reward: -21.0  Explore Rate: 0.6027415843082742  Frame Count: 124268\n",
            "INFO:Run: 103  Reward: -21.0  Explore Rate: 0.5997278763867329  Frame Count: 125610\n",
            "INFO:Run: 104  Reward: -20.0  Explore Rate: 0.5967292370047992  Frame Count: 126809\n",
            "INFO:Run: 105  Reward: -21.0  Explore Rate: 0.5937455908197752  Frame Count: 127821\n",
            "INFO:Run: 106  Reward: -20.0  Explore Rate: 0.5907768628656763  Frame Count: 128944\n",
            "INFO:Run: 107  Reward: -20.0  Explore Rate: 0.5878229785513479  Frame Count: 130160\n",
            "INFO:Run: 108  Reward: -21.0  Explore Rate: 0.5848838636585911  Frame Count: 131307\n",
            "INFO:Run: 109  Reward: -21.0  Explore Rate: 0.5819594443402982  Frame Count: 132473\n",
            "INFO:Run: 110  Reward: -17.0  Explore Rate: 0.5790496471185967  Frame Count: 134094\n",
            "INFO:Run: 111  Reward: -21.0  Explore Rate: 0.5761543988830038  Frame Count: 135111\n",
            "INFO:Run: 112  Reward: -21.0  Explore Rate: 0.5732736268885887  Frame Count: 136131\n",
            "INFO:Run: 113  Reward: -21.0  Explore Rate: 0.5704072587541458  Frame Count: 137335\n",
            "INFO:Run: 114  Reward: -21.0  Explore Rate: 0.567555222460375  Frame Count: 138351\n",
            "INFO:Run: 115  Reward: -21.0  Explore Rate: 0.5647174463480732  Frame Count: 139375\n",
            "INFO:Run: 116  Reward: -21.0  Explore Rate: 0.5618938591163328  Frame Count: 140516\n",
            "INFO:Run: 117  Reward: -21.0  Explore Rate: 0.5590843898207511  Frame Count: 141610\n",
            "INFO:Run: 118  Reward: -21.0  Explore Rate: 0.5562889678716474  Frame Count: 142702\n",
            "INFO:Run: 119  Reward: -21.0  Explore Rate: 0.5535075230322891  Frame Count: 143793\n",
            "INFO:Run: 120  Reward: -21.0  Explore Rate: 0.5507399854171277  Frame Count: 145063\n",
            "INFO:Run: 121  Reward: -21.0  Explore Rate: 0.547986285490042  Frame Count: 146177\n",
            "INFO:Run: 122  Reward: -21.0  Explore Rate: 0.5452463540625918  Frame Count: 147273\n",
            "INFO:Run: 123  Reward: -19.0  Explore Rate: 0.5425201222922789  Frame Count: 148592\n",
            "INFO:Run: 124  Reward: -20.0  Explore Rate: 0.5398075216808175  Frame Count: 149862\n",
            "INFO:Run: 125  Reward: -20.0  Explore Rate: 0.5371084840724134  Frame Count: 151039\n",
            "INFO:Run: 126  Reward: -20.0  Explore Rate: 0.5344229416520513  Frame Count: 152195\n",
            "INFO:Run: 127  Reward: -21.0  Explore Rate: 0.531750826943791  Frame Count: 153342\n",
            "INFO:Run: 128  Reward: -21.0  Explore Rate: 0.5290920728090721  Frame Count: 154399\n",
            "INFO:Run: 129  Reward: -21.0  Explore Rate: 0.5264466124450268  Frame Count: 155497\n",
            "INFO:Run: 130  Reward: -20.0  Explore Rate: 0.5238143793828016  Frame Count: 156614\n",
            "INFO:Run: 131  Reward: -21.0  Explore Rate: 0.5211953074858876  Frame Count: 157732\n",
            "INFO:Run: 132  Reward: -20.0  Explore Rate: 0.5185893309484582  Frame Count: 158899\n",
            "INFO:Run: 133  Reward: -21.0  Explore Rate: 0.5159963842937159  Frame Count: 160013\n",
            "INFO:Run: 134  Reward: -21.0  Explore Rate: 0.5134164023722473  Frame Count: 161110\n",
            "INFO:Run: 135  Reward: -20.0  Explore Rate: 0.510849320360386  Frame Count: 162226\n",
            "INFO:Run: 136  Reward: -20.0  Explore Rate: 0.5082950737585841  Frame Count: 163456\n",
            "INFO:Run: 137  Reward: -21.0  Explore Rate: 0.5057535983897912  Frame Count: 164646\n",
            "INFO:Run: 138  Reward: -21.0  Explore Rate: 0.5032248303978422  Frame Count: 165743\n",
            "INFO:Run: 139  Reward: -20.0  Explore Rate: 0.500708706245853  Frame Count: 166941\n",
            "INFO:Run: 140  Reward: -21.0  Explore Rate: 0.4982051627146237  Frame Count: 167963\n",
            "INFO:Run: 141  Reward: -21.0  Explore Rate: 0.49571413690105054  Frame Count: 169091\n",
            "INFO:Run: 142  Reward: -21.0  Explore Rate: 0.4932355662165453  Frame Count: 170116\n",
            "INFO:Run: 143  Reward: -21.0  Explore Rate: 0.4907693883854626  Frame Count: 171146\n",
            "INFO:Run: 144  Reward: -20.0  Explore Rate: 0.4883155414435353  Frame Count: 172417\n",
            "INFO:Run: 145  Reward: -19.0  Explore Rate: 0.4858739637363176  Frame Count: 173754\n",
            "INFO:Run: 146  Reward: -21.0  Explore Rate: 0.483444593917636  Frame Count: 174809\n",
            "INFO:Run: 147  Reward: -21.0  Explore Rate: 0.4810273709480478  Frame Count: 175898\n",
            "INFO:Run: 148  Reward: -21.0  Explore Rate: 0.47862223409330756  Frame Count: 176946\n",
            "INFO:Run: 149  Reward: -21.0  Explore Rate: 0.47622912292284103  Frame Count: 178195\n",
            "INFO:Run: 150  Reward: -20.0  Explore Rate: 0.4738479773082268  Frame Count: 179404\n",
            "INFO:Run: 151  Reward: -21.0  Explore Rate: 0.47147873742168567  Frame Count: 180417\n",
            "INFO:Run: 152  Reward: -21.0  Explore Rate: 0.46912134373457726  Frame Count: 181434\n",
            "INFO:Run: 153  Reward: -20.0  Explore Rate: 0.46677573701590436  Frame Count: 182594\n",
            "INFO:Run: 154  Reward: -21.0  Explore Rate: 0.46444185833082485  Frame Count: 183725\n",
            "INFO:Run: 155  Reward: -21.0  Explore Rate: 0.46211964903917074  Frame Count: 184777\n",
            "INFO:Run: 156  Reward: -20.0  Explore Rate: 0.4598090507939749  Frame Count: 186080\n",
            "INFO:Run: 157  Reward: -21.0  Explore Rate: 0.457510005540005  Frame Count: 187097\n",
            "INFO:Run: 158  Reward: -20.0  Explore Rate: 0.45522245551230495  Frame Count: 188299\n",
            "INFO:Run: 159  Reward: -20.0  Explore Rate: 0.4529463432347434  Frame Count: 189627\n",
            "INFO:Run: 160  Reward: -21.0  Explore Rate: 0.4506816115185697  Frame Count: 190789\n",
            "INFO:Run: 161  Reward: -21.0  Explore Rate: 0.4484282034609769  Frame Count: 191800\n",
            "INFO:Run: 162  Reward: -21.0  Explore Rate: 0.446186062443672  Frame Count: 192821\n",
            "INFO:Run: 163  Reward: -21.0  Explore Rate: 0.4439551321314536  Frame Count: 193846\n",
            "INFO:Run: 164  Reward: -21.0  Explore Rate: 0.4417353564707963  Frame Count: 194871\n",
            "INFO:Run: 165  Reward: -21.0  Explore Rate: 0.43952667968844233  Frame Count: 195903\n",
            "INFO:Run: 166  Reward: -21.0  Explore Rate: 0.43732904629000013  Frame Count: 196919\n",
            "INFO:Run: 167  Reward: -21.0  Explore Rate: 0.4351424010585501  Frame Count: 197940\n",
            "INFO:Run: 168  Reward: -21.0  Explore Rate: 0.43296668905325736  Frame Count: 198959\n",
            "INFO:Run: 169  Reward: -21.0  Explore Rate: 0.43080185560799106  Frame Count: 199982\n",
            "INFO:Run: 170  Reward: -20.0  Explore Rate: 0.4286478463299511  Frame Count: 201318\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-148-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-146-9fa098b2d6e8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_curr_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAIN_DQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0munprocessed_new_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-143-9069d2ea9018>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, session, state)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_explore_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}