{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Double DQN for Pong.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kopok/RL-Experiments/blob/master/DDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbEL4wFCiQS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN = True\n",
        "\n",
        "#ENV_NAME = 'BreakoutDeterministic-v4'\n",
        "ENV_NAME = 'PongDeterministic-v4'  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEVNwkbHiQTO",
        "colab_type": "text"
      },
      "source": [
        "# Deep Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PlJMyRKiQTX",
        "colab_type": "code",
        "outputId": "a64a0699-752c-45e5-dd1e-aa20956338f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"\n",
        "Implementation of DeepMind's Deep Q-Learning by Fabio M. Graetz, 2018\n",
        "If you have questions or suggestions, write me a mail fabiograetzatgooglemaildotcom\n",
        "\"\"\"\n",
        "import os\n",
        "import random\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import imageio\n",
        "from skimage.transform import resize\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5McRK6eiQTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FrameProcessor(object):\n",
        "    \"\"\"Resizes and converts RGB Atari frames to grayscale\"\"\"\n",
        "    def __init__(self, frame_height=84, frame_width=84):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            frame_height: Integer, Height of a frame of an Atari game\n",
        "            frame_width: Integer, Width of a frame of an Atari game\n",
        "        \"\"\"\n",
        "        self.frame_height = frame_height\n",
        "        self.frame_width = frame_width\n",
        "        self.frame = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
        "        self.processed = tf.image.rgb_to_grayscale(self.frame)\n",
        "        self.processed = tf.image.crop_to_bounding_box(self.processed, 34, 0, 160, 160)\n",
        "        self.processed = tf.image.resize_images(self.processed, \n",
        "                                                [self.frame_height, self.frame_width], \n",
        "                                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    \n",
        "    def __call__(self, session, frame):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            session: A Tensorflow session object\n",
        "            frame: A (210, 160, 3) frame of an Atari game in RGB\n",
        "        Returns:\n",
        "            A processed (84, 84, 1) frame in grayscale\n",
        "        \"\"\"\n",
        "        return session.run(self.processed, feed_dict={self.frame:frame})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t72AnRDuiQTn",
        "colab_type": "text"
      },
      "source": [
        "## 3. Dueling Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgmcQFRRiQTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQN(object):\n",
        "    \"\"\"Implements a Deep Q Network\"\"\"\n",
        "    \n",
        "    # pylint: disable=too-many-instance-attributes\n",
        "    \n",
        "    def __init__(self, n_actions, hidden=1024, learning_rate=0.00001, \n",
        "                 frame_height=84, frame_width=84, agent_history_length=4):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            n_actions: Integer, number of possible actions\n",
        "            hidden: Integer, Number of filters in the final convolutional layer. \n",
        "                    This is different from the DeepMind implementation\n",
        "            learning_rate: Float, Learning rate for the Adam optimizer\n",
        "            frame_height: Integer, Height of a frame of an Atari game\n",
        "            frame_width: Integer, Width of a frame of an Atari game\n",
        "            agent_history_length: Integer, Number of frames stacked together to create a state\n",
        "        \"\"\"\n",
        "        self.n_actions = n_actions\n",
        "        self.hidden = hidden\n",
        "        self.learning_rate = learning_rate\n",
        "        self.frame_height = frame_height\n",
        "        self.frame_width = frame_width\n",
        "        self.agent_history_length = agent_history_length\n",
        "        \n",
        "        self.input = tf.placeholder(shape=[None, self.frame_height, \n",
        "                                           self.frame_width, self.agent_history_length], \n",
        "                                    dtype=tf.float32)\n",
        "        # Normalizing the input\n",
        "        self.inputscaled = self.input/255\n",
        "        \n",
        "        # Convolutional layers\n",
        "        self.conv1 = tf.layers.conv2d(\n",
        "            inputs=self.inputscaled, filters=32, kernel_size=[8, 8], strides=4,\n",
        "            kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
        "            padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv1')\n",
        "        self.conv2 = tf.layers.conv2d(\n",
        "            inputs=self.conv1, filters=64, kernel_size=[4, 4], strides=2, \n",
        "            kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
        "            padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv2')\n",
        "        self.conv3 = tf.layers.conv2d(\n",
        "            inputs=self.conv2, filters=64, kernel_size=[3, 3], strides=1, \n",
        "            kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
        "            padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv3')\n",
        "        self.conv4 = tf.layers.conv2d(\n",
        "            inputs=self.conv3, filters=hidden, kernel_size=[7, 7], strides=1, \n",
        "            kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
        "            padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv4')\n",
        "        \n",
        "        \n",
        "\n",
        "        \n",
        "        # Splitting into value and advantage stream\n",
        "        self.valuestream, self.advantagestream = tf.split(self.conv4, 2, 3)\n",
        "        self.valuestream = tf.layers.flatten(self.valuestream)\n",
        "        self.advantagestream = tf.layers.flatten(self.advantagestream)\n",
        "        self.advantage = tf.layers.dense(\n",
        "            inputs=self.advantagestream, units=self.n_actions,\n",
        "            kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"advantage\")\n",
        "        self.value = tf.layers.dense(\n",
        "            inputs=self.valuestream, units=1, \n",
        "            kernel_initializer=tf.variance_scaling_initializer(scale=2), name='value')\n",
        "        \n",
        "        # Combining value and advantage into Q-values as described above\n",
        "        self.q_values = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage, axis=1, keepdims=True))\n",
        "        self.best_action = tf.argmax(self.q_values, 1)\n",
        "        \n",
        "        # The next lines perform the parameter update. \n",
        "        \n",
        "        # targetQ according to Bellman equation: \n",
        "        # Q = r + gamma*max Q', calculated in the function learn()\n",
        "        self.target_q = tf.placeholder(shape=[None], dtype=tf.float32)\n",
        "        # Action that was performed\n",
        "        self.action = tf.placeholder(shape=[None], dtype=tf.int32)\n",
        "        # Q value of the action that was performed\n",
        "        self.Q = tf.reduce_sum(tf.multiply(self.q_values, tf.one_hot(self.action, self.n_actions, dtype=tf.float32)), axis=1)\n",
        "        \n",
        "        # Parameter updates\n",
        "        self.loss = tf.reduce_mean(tf.losses.huber_loss(labels=self.target_q, predictions=self.Q))\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "        self.update = self.optimizer.minimize(self.loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2cSO_XniQT1",
        "colab_type": "text"
      },
      "source": [
        "## 4. Exploration-exploitation trade-off\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf4PoNhliQT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ExplorationExploitationScheduler(object):\n",
        "    \"\"\"Determines an action according to an epsilon greedy strategy with annealing epsilon\"\"\"\n",
        "    def __init__(self, DQN, n_actions, eps_initial=1, eps_final=0.1, eps_final_frame=0.01, \n",
        "                 eps_evaluation=0.0, eps_annealing_frames=1000000, \n",
        "                 replay_memory_start_size=50000, max_frames=25000000):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            DQN: A DQN object\n",
        "            n_actions: Integer, number of possible actions\n",
        "            eps_initial: Float, Exploration probability for the first \n",
        "                replay_memory_start_size frames\n",
        "            eps_final: Float, Exploration probability after \n",
        "                replay_memory_start_size + eps_annealing_frames frames\n",
        "            eps_final_frame: Float, Exploration probability after max_frames frames\n",
        "            eps_evaluation: Float, Exploration probability during evaluation\n",
        "            eps_annealing_frames: Int, Number of frames over which the \n",
        "                exploration probabilty is annealed from eps_initial to eps_final\n",
        "            replay_memory_start_size: Integer, Number of frames during \n",
        "                which the agent only explores\n",
        "            max_frames: Integer, Total number of frames shown to the agent\n",
        "        \"\"\"\n",
        "        self.n_actions = n_actions\n",
        "        self.eps_initial = eps_initial\n",
        "        self.eps_final = eps_final\n",
        "        self.eps_final_frame = eps_final_frame\n",
        "        self.eps_evaluation = eps_evaluation\n",
        "        self.eps_annealing_frames = eps_annealing_frames\n",
        "        self.replay_memory_start_size = replay_memory_start_size\n",
        "        self.max_frames = max_frames\n",
        "        \n",
        "        # Slopes and intercepts for exploration decrease\n",
        "        self.slope = -(self.eps_initial - self.eps_final)/self.eps_annealing_frames\n",
        "        self.intercept = self.eps_initial - self.slope*self.replay_memory_start_size\n",
        "        self.slope_2 = -(self.eps_final - self.eps_final_frame)/(self.max_frames - self.eps_annealing_frames - self.replay_memory_start_size)\n",
        "        self.intercept_2 = self.eps_final_frame - self.slope_2*self.max_frames\n",
        "        \n",
        "        self.DQN = DQN\n",
        "\n",
        "    def get_epsilon(self, frame_number, evaluation=False):\n",
        "        if evaluation:\n",
        "            eps = self.eps_evaluation\n",
        "        elif frame_number < self.replay_memory_start_size:\n",
        "            eps = self.eps_initial\n",
        "        elif frame_number >= self.replay_memory_start_size and frame_number < self.replay_memory_start_size + self.eps_annealing_frames:\n",
        "            eps = self.slope*frame_number + self.intercept\n",
        "        elif frame_number >= self.replay_memory_start_size + self.eps_annealing_frames:\n",
        "            eps = self.slope_2*frame_number + self.intercept_2\n",
        "        return eps\n",
        "\n",
        "    def get_action(self, session, frame_number, state, evaluation=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            session: A tensorflow session object\n",
        "            frame_number: Integer, number of the current frame\n",
        "            state: A (84, 84, 4) sequence of frames of an Atari game in grayscale\n",
        "            evaluation: A boolean saying whether the agent is being evaluated\n",
        "        Returns:\n",
        "            An integer between 0 and n_actions - 1 determining the action the agent perfoms next\n",
        "        \"\"\"\n",
        "        \n",
        "        \n",
        "        if np.random.rand(1) < self.get_epsilon(frame_number, evaluation):\n",
        "            return np.random.randint(0, self.n_actions)\n",
        "        return session.run(self.DQN.best_action, feed_dict={self.DQN.input:[state]})[0]  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGaya6444T3i",
        "colab_type": "text"
      },
      "source": [
        "## Replay Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2mmMyBxiQT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayMemory(object):\n",
        "    \"\"\"Replay Memory that stores the last size=1,000,000 transitions\"\"\"\n",
        "    def __init__(self, size=1000000, frame_height=84, frame_width=84, \n",
        "                 agent_history_length=4, batch_size=32):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            size: Integer, Number of stored transitions\n",
        "            frame_height: Integer, Height of a frame of an Atari game\n",
        "            frame_width: Integer, Width of a frame of an Atari game\n",
        "            agent_history_length: Integer, Number of frames stacked together to create a state\n",
        "            batch_size: Integer, Number if transitions returned in a minibatch\n",
        "        \"\"\"\n",
        "        self.size = size\n",
        "        self.frame_height = frame_height\n",
        "        self.frame_width = frame_width\n",
        "        self.agent_history_length = agent_history_length\n",
        "        self.batch_size = batch_size\n",
        "        self.count = 0\n",
        "        self.current = 0\n",
        "        \n",
        "        # Pre-allocate memory\n",
        "        self.actions = np.empty(self.size, dtype=np.int32)\n",
        "        self.rewards = np.empty(self.size, dtype=np.float32)\n",
        "        self.frames = np.empty((self.size, self.frame_height, self.frame_width), dtype=np.uint8)\n",
        "        self.terminal_flags = np.empty(self.size, dtype=np.bool)\n",
        "        \n",
        "        # Pre-allocate memory for the states and new_states in a minibatch\n",
        "        self.states = np.empty((self.batch_size, self.agent_history_length, \n",
        "                                self.frame_height, self.frame_width), dtype=np.uint8)\n",
        "        self.new_states = np.empty((self.batch_size, self.agent_history_length, \n",
        "                                    self.frame_height, self.frame_width), dtype=np.uint8)\n",
        "        self.indices = np.empty(self.batch_size, dtype=np.int32)\n",
        "        \n",
        "\n",
        "    def load(self, arr):\n",
        "        self.actions = arr[0]\n",
        "        self.rewards = arr[1]\n",
        "        self.frames = arr[2]\n",
        "        \n",
        "        self.terminal_flags = arr[3]\n",
        "        self.states = arr[4]\n",
        "        self.new_states = arr[5]\n",
        "        self.indices = arr[6]\n",
        "\n",
        "        self.count=len(self.actions)\n",
        "        self.current = (self.count + 1) % self.size\n",
        "\n",
        "    def add_experience(self, action, frame, reward, terminal):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            action: An integer between 0 and env.action_space.n - 1 \n",
        "                determining the action the agent perfomed\n",
        "            frame: A (84, 84, 1) frame of an Atari game in grayscale\n",
        "            reward: A float determining the reward the agend received for performing an action\n",
        "            terminal: A bool stating whether the episode terminated\n",
        "        \"\"\"\n",
        "        if frame.shape != (self.frame_height, self.frame_width):\n",
        "            raise ValueError('Dimension of frame is wrong!')\n",
        "        self.actions[self.current] = action\n",
        "        self.frames[self.current, ...] = frame\n",
        "        self.rewards[self.current] = reward\n",
        "        self.terminal_flags[self.current] = terminal\n",
        "        self.count = max(self.count, self.current+1)\n",
        "        self.current = (self.current + 1) % self.size\n",
        "             \n",
        "    def _get_state(self, index):\n",
        "        if self.count is 0:\n",
        "            raise ValueError(\"The replay memory is empty!\")\n",
        "        if index < self.agent_history_length - 1:\n",
        "            raise ValueError(\"Index must be min 3\")\n",
        "        return self.frames[index-self.agent_history_length+1:index+1, ...]\n",
        "        \n",
        "    def _get_valid_indices(self):\n",
        "        for i in range(self.batch_size):\n",
        "            while True:\n",
        "                index = random.randint(self.agent_history_length, self.count - 1)\n",
        "                if index < self.agent_history_length:\n",
        "                    continue\n",
        "                if index >= self.current and index - self.agent_history_length <= self.current:\n",
        "                    continue\n",
        "                if self.terminal_flags[index - self.agent_history_length:index].any():\n",
        "                    continue\n",
        "                break\n",
        "            self.indices[i] = index\n",
        "            \n",
        "    def get_minibatch(self):\n",
        "        \"\"\"\n",
        "        Returns a minibatch of self.batch_size = 32 transitions\n",
        "        \"\"\"\n",
        "        if self.count < self.agent_history_length:\n",
        "            raise ValueError('Not enough memories to get a minibatch')\n",
        "        \n",
        "        self._get_valid_indices()\n",
        "            \n",
        "        for i, idx in enumerate(self.indices):\n",
        "            self.states[i] = self._get_state(idx - 1)\n",
        "            self.new_states[i] = self._get_state(idx)\n",
        "        \n",
        "        return np.transpose(self.states, axes=(0, 2, 3, 1)), self.actions[self.indices], self.rewards[self.indices], np.transpose(self.new_states, axes=(0, 2, 3, 1)), self.terminal_flags[self.indices]\n",
        "                "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2opluCBiQUH",
        "colab_type": "text"
      },
      "source": [
        "## 6. Target network and parameter update\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns-W8ycDiQUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learn(session, replay_memory, main_dqn, target_dqn, batch_size, gamma):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        session: A tensorflow sesson object\n",
        "        replay_memory: A ReplayMemory object\n",
        "        main_dqn: A DQN object\n",
        "        target_dqn: A DQN object\n",
        "        batch_size: Integer, Batch size\n",
        "        gamma: Float, discount factor for the Bellman equation\n",
        "    Returns:\n",
        "        loss: The loss of the minibatch, for tensorboard\n",
        "    Draws a minibatch from the replay memory, calculates the \n",
        "    target Q-value that the prediction Q-value is regressed to. \n",
        "    Then a parameter update is performed on the main DQN.\n",
        "    \"\"\"\n",
        "    # Draw a minibatch from the replay memory\n",
        "    states, actions, rewards, new_states, terminal_flags = replay_memory.get_minibatch()    \n",
        "    # The main network estimates which action is best (in the next \n",
        "    # state s', new_states is passed!) \n",
        "    # for every transition in the minibatch\n",
        "\n",
        "    q_values = session.run(main_dqn.q_values, feed_dict={main_dqn.input:new_states})\n",
        "\n",
        "    arg_q_max = np.argmax(q_values, axis=1)\n",
        "    # The target network estimates the Q-values (in the next state s', new_states is passed!) \n",
        "    # for every transition in the minibatch\n",
        "    q_vals = session.run(target_dqn.q_values, feed_dict={target_dqn.input:new_states})\n",
        "    double_q = q_vals[range(batch_size), arg_q_max]\n",
        "    # Bellman equation. Multiplication with (1-terminal_flags) makes sure that \n",
        "    # if the game is over, targetQ=rewards\n",
        "    target_q = rewards + (gamma*double_q * (1-terminal_flags))\n",
        "    # Gradient descend step to update the parameters of the main network\n",
        "    loss, _ = session.run([main_dqn.loss, main_dqn.update], \n",
        "                          feed_dict={main_dqn.input:states, \n",
        "                                     main_dqn.target_q:target_q, \n",
        "                                     main_dqn.action:actions})\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1csJAU8iQUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TargetNetworkUpdater(object):\n",
        "    \"\"\"Copies the parameters of the main DQN to the target DQN\"\"\"\n",
        "    def __init__(self, main_dqn_vars, target_dqn_vars):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            main_dqn_vars: A list of tensorflow variables belonging to the main DQN network\n",
        "            target_dqn_vars: A list of tensorflow variables belonging to the target DQN network\n",
        "        \"\"\"\n",
        "        self.main_dqn_vars = main_dqn_vars\n",
        "        self.target_dqn_vars = target_dqn_vars\n",
        "\n",
        "    def _update_target_vars(self):\n",
        "        update_ops = []\n",
        "        for i, var in enumerate(self.main_dqn_vars):\n",
        "            copy_op = self.target_dqn_vars[i].assign(var.value())\n",
        "            update_ops.append(copy_op)\n",
        "        return update_ops\n",
        "            \n",
        "    def __call__(self, sess):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sess: A Tensorflow session object\n",
        "        Assigns the values of the parameters of the main network to the \n",
        "        parameters of the target network\n",
        "        \"\"\"\n",
        "        update_ops = self._update_target_vars()\n",
        "        for copy_op in update_ops:\n",
        "            sess.run(copy_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wKNj810iQUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_gif(frame_number, frames_for_gif, reward, path):\n",
        "    \"\"\"\n",
        "        Args:\n",
        "            frame_number: Integer, determining the number of the current frame\n",
        "            frames_for_gif: A sequence of (210, 160, 3) frames of an Atari game in RGB\n",
        "            reward: Integer, Total reward of the episode that es ouputted as a gif\n",
        "            path: String, path where gif is saved\n",
        "    \"\"\"\n",
        "    for idx, frame_idx in enumerate(frames_for_gif): \n",
        "        frames_for_gif[idx] = resize(frame_idx, (420, 320, 3), \n",
        "                                     preserve_range=True, order=0).astype(np.uint8)\n",
        "        \n",
        "    imageio.mimsave(f'{path}{\"ATARI_frame_{0}_reward_{1}.gif\".format(frame_number, reward)}', \n",
        "                    frames_for_gif, duration=1/30)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbQGds3riQUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Atari(object):\n",
        "    \"\"\"Wrapper for the environment provided by gym\"\"\"\n",
        "    def __init__(self, envName, no_op_steps=10, agent_history_length=4):\n",
        "        self.env = gym.make(envName)\n",
        "        self.process_frame = FrameProcessor()\n",
        "        self.state = None\n",
        "        self.last_lives = 0\n",
        "        self.no_op_steps = no_op_steps\n",
        "        self.agent_history_length = agent_history_length\n",
        "\n",
        "    def reset(self, sess, evaluation=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sess: A Tensorflow session object\n",
        "            evaluation: A boolean saying whether the agent is evaluating or training\n",
        "        Resets the environment and stacks four frames ontop of each other to \n",
        "        create the first state\n",
        "        \"\"\"\n",
        "        frame = self.env.reset()\n",
        "        self.last_lives = 0\n",
        "        terminal_life_lost = True # Set to true so that the agent starts \n",
        "                                  # with a 'FIRE' action when evaluating\n",
        "        if evaluation:\n",
        "            for _ in range(random.randint(1, self.no_op_steps)):\n",
        "                frame, _, _, _ = self.env.step(1) # Action 'Fire'\n",
        "        processed_frame = self.process_frame(sess, frame)   # (★★★)\n",
        "        self.state = np.repeat(processed_frame, self.agent_history_length, axis=2)\n",
        "        \n",
        "        return terminal_life_lost\n",
        "\n",
        "    def step(self, sess, action):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sess: A Tensorflow session object\n",
        "            action: Integer, action the agent performs\n",
        "        Performs an action and observes the reward and terminal state from the environment\n",
        "        \"\"\"\n",
        "        new_frame, reward, terminal, info = self.env.step(action)  # (5★)\n",
        "            \n",
        "        if info['ale.lives'] < self.last_lives:\n",
        "            terminal_life_lost = True\n",
        "        else:\n",
        "            terminal_life_lost = terminal\n",
        "        self.last_lives = info['ale.lives']\n",
        "        \n",
        "        processed_new_frame = self.process_frame(sess, new_frame)   # (6★)\n",
        "        new_state = np.append(self.state[:, :, 1:], processed_new_frame, axis=2) # (6★)   \n",
        "        self.state = new_state\n",
        "        \n",
        "        return processed_new_frame, reward, terminal, terminal_life_lost, new_frame\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euxzj4GLiQUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clip_reward(reward):\n",
        "    if reward > 0:\n",
        "        return 1\n",
        "    elif reward == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLkoxsthiQU0",
        "colab_type": "code",
        "outputId": "08af1f41-cfc2-449b-d756-86388fb2f393",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "# Control parameters\n",
        "MAX_EPISODE_LENGTH = 18000       # Equivalent of 5 minutes of gameplay at 60 frames per second\n",
        "EVAL_FREQUENCY = 50000          # Number of frames the agent sees between evaluations\n",
        "EVAL_STEPS = 10000               # Number of frames for one evaluation\n",
        "NETW_UPDATE_FREQ = 10000         # Number of chosen actions between updating the target network. \n",
        "                                 # According to Mnih et al. 2015 this is measured in the number of \n",
        "                                 # parameter updates (every four actions), however, in the \n",
        "                                 # DeepMind code, it is clearly measured in the number\n",
        "                                 # of actions the agent choses\n",
        "DISCOUNT_FACTOR = 0.99           # gamma in the Bellman equation\n",
        "REPLAY_MEMORY_START_SIZE = 100 # Number of completely random actions, \n",
        "                                 # before the agent starts learning\n",
        "MAX_FRAMES = 30000000            # Total number of frames the agent sees \n",
        "MEMORY_SIZE = 300000             # Number of transitions stored in the replay memory\n",
        "NO_OP_STEPS = 10                 # Number of 'NOOP' or 'FIRE' actions at the beginning of an \n",
        "                                 # evaluation episode\n",
        "UPDATE_FREQ = 4                  # Every four actions a gradient descend step is performed\n",
        "HIDDEN = 1024                    # Number of filters in the final convolutional layer. The output \n",
        "                                 # has the shape (1,1,1024) which is split into two streams. Both \n",
        "                                 # the advantage stream and value stream have the shape \n",
        "                                 # (1,1,512). This is slightly different from the original \n",
        "                                 # implementation but tests I did with the environment Pong \n",
        "                                 # have shown that this way the score increases more quickly\n",
        "LEARNING_RATE = 0.00025          # Set to 0.00025 in Pong for quicker results. \n",
        "                                 # Hessel et al. 2017 used 0.0000625\n",
        "BS = 32                          # Batch size\n",
        "\n",
        "PATH = \"/content/gdrive/My Drive/Models/checks/\"                 # Gifs and checkpoints will be saved here\n",
        "RUNID = 'run_1'\n",
        "\n",
        "atari = Atari(ENV_NAME, NO_OP_STEPS)\n",
        "\n",
        "print(\"The environment has the following {} actions: {}\".format(atari.env.action_space.n, \n",
        "                                                                atari.env.unwrapped.get_action_meanings()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The environment has the following 6 actions: ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DnxZdEQiQU6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# main DQN and target DQN networks:\n",
        "with tf.variable_scope('mainDQN'):\n",
        "    MAIN_DQN = DQN(atari.env.action_space.n, HIDDEN, LEARNING_RATE)   # (★★)\n",
        "with tf.variable_scope('targetDQN'):\n",
        "    TARGET_DQN = DQN(atari.env.action_space.n, HIDDEN)               # (★★)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()    \n",
        "\n",
        "MAIN_DQN_VARS = tf.trainable_variables(scope='mainDQN')\n",
        "TARGET_DQN_VARS = tf.trainable_variables(scope='targetDQN')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enQgQP-miQVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    \"\"\"Contains the training and evaluation loops\"\"\"\n",
        "    my_replay_memory = ReplayMemory(size=MEMORY_SIZE, batch_size=BS)   # (★)\n",
        "    update_networks = TargetNetworkUpdater(MAIN_DQN_VARS, TARGET_DQN_VARS)\n",
        "    \n",
        "    explore_exploit_sched = ExplorationExploitationScheduler(\n",
        "        MAIN_DQN, atari.env.action_space.n, \n",
        "        replay_memory_start_size=REPLAY_MEMORY_START_SIZE, \n",
        "        max_frames=MAX_FRAMES)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "\n",
        "        my_replay_memory.load(np.load(PATH+\"Memory.npy\",allow_pickle=True))\n",
        "        print(\"Imported Memory\")\n",
        "        saver = tf.train.import_meta_graph(PATH+\"1667-3065094.meta\")\n",
        "        saver.restore(sess,tf.train.latest_checkpoint(PATH))\n",
        "        \n",
        "        sess.run(init)\n",
        "        frame_number = 3065094\n",
        "        rewards = []\n",
        "        loss_list = []\n",
        "        run = 1667\n",
        "\n",
        "        while frame_number < MAX_FRAMES:#\n",
        "            \n",
        "            epoch_frame = 0\n",
        "            while epoch_frame < EVAL_FREQUENCY:\n",
        "                run+=1\n",
        "                terminal_life_lost = atari.reset(sess)\n",
        "                episode_reward_sum = 0\n",
        "                for _ in range(MAX_EPISODE_LENGTH):\n",
        "                    # (4★)\n",
        "                    action = explore_exploit_sched.get_action(sess, frame_number, atari.state)   \n",
        "                    # (5★)\n",
        "                    processed_new_frame, reward, terminal, terminal_life_lost, _ = atari.step(sess, action)  \n",
        "                    frame_number += 1\n",
        "                    epoch_frame += 1\n",
        "                    episode_reward_sum += reward\n",
        "                    \n",
        "                    # Clip the reward\n",
        "                    clipped_reward = clip_reward(reward)\n",
        "                    \n",
        "                    # (7★) Store transition in the replay memory\n",
        "                    my_replay_memory.add_experience(action=action, \n",
        "                                                    frame=processed_new_frame[:, :, 0],\n",
        "                                                    reward=clipped_reward, \n",
        "                                                    terminal=terminal_life_lost)   \n",
        "                    \n",
        "                    if frame_number % UPDATE_FREQ == 0 and frame_number > REPLAY_MEMORY_START_SIZE:\n",
        "                        loss = learn(sess, my_replay_memory, MAIN_DQN, TARGET_DQN,\n",
        "                                     BS, gamma = DISCOUNT_FACTOR) # (8★)\n",
        "                        loss_list.append(loss)\n",
        "                    if frame_number % NETW_UPDATE_FREQ == 0 and frame_number > REPLAY_MEMORY_START_SIZE:\n",
        "                        update_networks(sess) # (9★)\n",
        "                    \n",
        "                    if terminal:\n",
        "                        print(\"Run: \" + str(run) + \"  Reward: \" + str(episode_reward_sum) + \"  Explore Rate: \" + str(explore_exploit_sched.get_epsilon(frame_number)) + \"  Frame Count: \"+ str(frame_number))\n",
        "                        terminal = False\n",
        "                        break\n",
        "\n",
        "                rewards.append(episode_reward_sum)\n",
        "           \n",
        "            #Save the network parameters\n",
        "            saver.save(sess, PATH+str(run), global_step=frame_number)\n",
        "            np.save(PATH+\"Memory\", np.array([my_replay_memory.actions , my_replay_memory.rewards ,  my_replay_memory.frames ,   my_replay_memory.terminal_flags ,my_replay_memory.states , my_replay_memory.new_states , my_replay_memory.indices] ))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNqUblgC6A-z",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_viWfvKDiQVN",
        "colab_type": "code",
        "outputId": "7664b2a0-5444-43a9-f671-1642f7df064b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "if TRAIN:\n",
        "    train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Imported Memory\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/Models/checks/1714-3167500\n",
            "Run: 1668  Reward: -21.0  Explore Rate: 0.09358872547836372  Frame Count: 3065948\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmpWxtC_iQVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_files_dict = {\n",
        "    'BreakoutDeterministic-v4':(\"trained/breakout/\", \"my_model-15845555.meta\"),\n",
        "    'PongDeterministic-v4':(\"trained/pong/\", \"my_model-3217770.meta\")\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3uQWiEriQVY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not TRAIN:\n",
        "    \n",
        "    gif_path = \"GIF/\"\n",
        "    os.makedirs(gif_path, exist_ok=True)\n",
        "\n",
        "    trained_path, save_file = save_files_dict[ENV_NAME]\n",
        "\n",
        "    explore_exploit_sched = ExplorationExploitationScheduler(\n",
        "        MAIN_DQN, atari.env.action_space.n, \n",
        "        replay_memory_start_size=REPLAY_MEMORY_START_SIZE, \n",
        "        max_frames=MAX_FRAMES)\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "        saver = tf.train.import_meta_graph(trained_path+save_file)\n",
        "        saver.restore(sess,tf.train.latest_checkpoint(trained_path))\n",
        "        frames_for_gif = []\n",
        "        terminal_life_lost = atari.reset(sess, evaluation = True)\n",
        "        episode_reward_sum = 0\n",
        "        while True:\n",
        "            atari.env.render()\n",
        "            action = 1 if terminal_life_lost else explore_exploit_sched.get_action(sess, 0, atari.state,  \n",
        "                                                                                   evaluation = True)\n",
        "            \n",
        "            processed_new_frame, reward, terminal, terminal_life_lost, new_frame = atari.step(sess, action)\n",
        "            episode_reward_sum += reward\n",
        "            frames_for_gif.append(new_frame)\n",
        "            if terminal == True:\n",
        "                break\n",
        "        \n",
        "        atari.env.close()\n",
        "        print(\"The total reward is {}\".format(episode_reward_sum))\n",
        "        print(\"Creating gif...\")\n",
        "        generate_gif(0, frames_for_gif, episode_reward_sum, gif_path)\n",
        "        print(\"Gif created, check the folder {}\".format(gif_path))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}